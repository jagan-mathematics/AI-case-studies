{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f154cf1-cc59-41a2-85d4-73c25f955f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Every possible byte (really an integer 0..255) gets mapped by OpenAI to a unicode\n",
    "    character that represents it visually. Some bytes have their appearance preserved\n",
    "    because they don't cause any trouble. These are defined in list bs. For example:\n",
    "    chr(33) returns \"!\", so in the returned dictionary we simply have d[33] -> \"!\".\n",
    "    However, chr(0), for example, is '\\x00', which looks ugly. So OpenAI maps these\n",
    "    bytes, into new characters in a range where chr() returns a single nice character.\n",
    "    So in the final dictionary we have d[0] -> 'Ā' instead, which is just chr(0 + 2**8).\n",
    "    In particular, the space character is 32, which we can see by ord(' '). Instead,\n",
    "    this function will shift space (32) by 256 to 288, so d[32] -> 'Ġ'.\n",
    "    So this is just a simple one-to-one mapping of bytes 0..255 into unicode characters\n",
    "    that \"look nice\", either in their original form, or a funny shifted character\n",
    "    like 'Ā', or 'Ġ', etc.\n",
    "    \"\"\"\n",
    "    # the 188 integers that render fine in their original form and need no shifting\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:] # all integers b in bs will simply map to chr(b) in the output dict\n",
    "    # now get the representations of the other 68 integers that do need shifting\n",
    "    # each will get mapped chr(256 + n), where n will grow from 0...67 in the loop\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            # if this byte is \"ugly\" then map it to the next available \"nice\" character\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, cs))\n",
    "    return d\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return all bigrams as a set of tuples, of consecutive elements in the iterable word.\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5665787-5a53-4d69-a4d5-b4aa9d851b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_local_file = \"trained_tokenizer/tokenizer.json\"\n",
    "\n",
    "with open(encoder_local_file, 'r') as f:\n",
    "    encoder = json.load(f)\n",
    "# load encoder.json that has the raw mappings from token -> bpe index\n",
    "\n",
    "encode = encoder[\"model\"][\"vocab\"]\n",
    "bpe_data = encoder[\"model\"][\"merges\"]\n",
    "# light postprocessing: strip the version on first line and the last line is a blank\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcacf2d3-9b4b-4e13-a47d-a80172f7c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_encoder = bytes_to_unicode()\n",
    "\n",
    "def normalize_value(value):\n",
    "    return \"\".join([byte_encoder[i] for i in list(value.encode(\"utf-8\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5ce1f4-61b3-404e-b253-b4ab6e66330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for i in range(11):\n",
    "    if i == 0:\n",
    "        value = \"00\" \n",
    "    else:\n",
    "        value = \"0\"*(1 - int(math.log10(i))) + f\"{i}\"\n",
    "    value = normalize_value(value)\n",
    "    encode[value] = max(list(encode.values())) + 1\n",
    "    bpe_merges.append((value[:-1], value[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b240fd-3226-487f-8166-ea95ef2870c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(11, 100):\n",
    "    value = \"0\"*(1 - int(math.log10(i))) + f\"{i}\"\n",
    "    value = normalize_value(value)\n",
    "    encode[value] = max(list(encode.values())) + 1\n",
    "    bpe_merges.append((value[:-1], value[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca7d6aa-5e5c-4c53-b909-403e7ce4582f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(101):\n",
    "    if i == 0:\n",
    "        value = \"000\"\n",
    "    else:\n",
    "        value = \"0\"*(2 - int(math.log10(i))) + f\"{i}\"\n",
    "    value = normalize_value(value)\n",
    "    encode[value] = max(list(encode.values())) + 1\n",
    "    bpe_merges.append((value[:-1], value[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0f1441-0ed8-4d89-815c-71fa249319f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(101, 1000):\n",
    "    value = \"0\"*(2 - int(math.log10(i))) + f\"{i}\"\n",
    "    value = normalize_value(value)\n",
    "    encode[value] = max(list(encode.values())) + 1\n",
    "    bpe_merges.append((value[:-1], value[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2fea41-2f03-4cf3-aa82-8e2e3c7f4760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder[\"model\"][\"merges\"] = [\" \".join(i) for i in bpe_merges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0175f4f5-e879-4474-a753-a1eacc362b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder[\"model\"][\"vocab\"] = encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e5a3860-70d2-4f3f-bc4b-536f3f16696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"trained_tokenizer/post_processed_tokenizer.json\", 'w') as f:\n",
    "    json.dump(encoder, f)\n",
    "# load encoder.json that has the raw map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
